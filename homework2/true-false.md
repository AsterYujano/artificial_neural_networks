# True-False Questions

## Different layers of a deep network learn at different speeds because their effects on the output are different. 

## Two hidden layers are sufficient to approximate any real-valued function with NNN inputs and one output in terms of a perceptron.

## In minimisation with a Lagrange multiplier, the function multiplying the Lagrange multiplier must be equal to or larger than zero. 

## Using a stochastic path through weight space in backpropagation allows for the energy to increase in some updates. 

## Some of the functions with 5 Boolean valued inputs and one Boolean valued output are linearly separable. 

## Weight decay helps against overfitting. 

## Using g(b)=bg(b) = bg(b)=b as activation function and setting all thresholds to zero in a multilayer perceptron allows you to solve some linearly inseparable problems.

## The number of NNN-dimensional Boolean functions is larger than 2^N

## Nesterov's accelerated gradient scheme is often more efficient than the simple momentum scheme because the weighting factor of the momentum term increases as a function of iteration number. 